{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc37312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kayhan/opt/anaconda3/envs/pendulum/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 656/656 [00:00<00:00, 89.3kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 479M/479M [00:06<00:00, 77.0MB/s]\n",
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)neration_config.json: 100%|███| 74.0/74.0 [00:00<00:00, 3.19kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 816k/816k [00:00<00:00, 2.41MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 458k/458k [00:00<00:00, 1.39MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 1.27M/1.27M [00:00<00:00, 2.75MB/s]\n",
      "/Users/kayhan/opt/anaconda3/envs/pendulum/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model,'he said, when i was finished.'ah well,'said the man,'that's\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, \" she said. \\n she reached the bottom of the shaft and leaned a little further out. it was'},\n",
       " {'generated_text': 'Hello, I\\'m a language model, \" she laughed. \" we call that a\\'white girl.\\'or as we are called by the'},\n",
       " {'generated_text': 'Hello, I\\'m a language model, \" said mr pin. \" an\\'the ones with the funny hats don\\'t. \" the rest of'},\n",
       " {'generated_text': 'Hello, I\\'m a language model, was\\'ere \\'bout to do some more dancin \\', \" he said, then his voice lowered to'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='openai-gpt')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\",\n",
    "          max_length=100, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b07bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.read_csv('50_review_result.csv')\n",
    "texts = np.array(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43292000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = generator(\"This restaurant was lovely,\", max_length=100, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c3e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grouped_review'] = df.groupby(by=['name'])['review'].transform(lambda x: ','.join(x))\n",
    "concatted = df[['name','grouped_review']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dcd748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in concatted.iterrows():\n",
    "    print(row['grouped_review'])\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b66f091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = concatted['grouped_review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12c2d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kayhan/opt/anaconda3/envs/pendulum/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|███| 27.0/27.0 [00:00<00:00, 2.49kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██| 1.94k/1.94k [00:00<00:00, 286kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 798k/798k [00:00<00:00, 2.37MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 1.36MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|███| 20.0/20.0 [00:00<00:00, 4.22kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 283/283 [00:00<00:00, 17.7kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|█| 1.79G/1.79G [00:18<00:00, 94.9MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 197/197 [00:00<00:00, 11.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, LEDConfig, LEDForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/PRIMERA')\n",
    "\n",
    "config=LEDConfig.from_pretrained('allenai/PRIMERA')\n",
    "\n",
    "model = LEDForConditionalGeneration.from_pretrained('allenai/PRIMERA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "692f1609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dumpling Kitchen                  158\n",
       "CreoLa                            100\n",
       "Cafe De Casa                      100\n",
       "Automat                           100\n",
       "Nopa                              100\n",
       "Pearl                             100\n",
       "Wooden Spoon                      100\n",
       "Routier                           100\n",
       "Limoncello                        100\n",
       "Sweet Maple                       100\n",
       "Alnico                            100\n",
       "The Italian Homemade Company      100\n",
       "Horsefeather                      100\n",
       "Daeho Kalbijjim & Beef Soup       100\n",
       "Breadbelly                        100\n",
       "San Tung                          100\n",
       "Dumpling Home                     100\n",
       "Farmhouse Kitchen Thai Cuisine    100\n",
       "Fable                             100\n",
       "Um Ma                             100\n",
       "The Tailor's Son                  100\n",
       "Santeria                          100\n",
       "Dumpling House                    100\n",
       "Starbelly                         100\n",
       "Marufuku Ramen                    100\n",
       "Bottega                           100\n",
       "Lily                              100\n",
       "Beit Rima                         100\n",
       "The Snug                          100\n",
       "Purple Rice                       100\n",
       "KAIYŌ Rooftop                     100\n",
       "Jamie's Place                     100\n",
       "Sotto Mare                        100\n",
       "JIJIME                            100\n",
       "Tanglad                           100\n",
       "Otra                              100\n",
       "Lokma                             100\n",
       "Beretta                           100\n",
       "Fog Harbor Fish House              90\n",
       "Savor                              88\n",
       "Gao Viet Kitchen & Bar             78\n",
       "Noodle in a Haystack               60\n",
       "The Laundromat                     55\n",
       "That's My Jam                      42\n",
       "Kothai Republic                    37\n",
       "Hu Tong Jian Bing                  36\n",
       "Pinsa Rossa                        28\n",
       "Arang                              25\n",
       "Rise Over Run                      21\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02318ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = df[df['name'] == 'Marufuku Ramen']['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55338f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = ','.join(test2)[:16384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da1a75e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The noodle was very good I got the Tonkatsu Ramen Noodles and the staff was very respectful and nice and overall I recommended this place I would definitely come back here again,My favorite ramen place in town! I always get the ramen with chicken where they give you a whole cooked peace of chicken on the side. It comes on a frying pen and is amazing. Can not go wrong with this place in my opinion.,I 100% will go there again.The ramen was so good and the best part of the food came out in seconds.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0d9f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.encode(test2,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "357bcb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24400"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "437f0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LEDForConditionalGeneration\n",
    "c\n",
    "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d832734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARTICLE_TO_SUMMARIZE = test2\n",
    "inputs = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\")\n",
    "\n",
    "# Global attention on the first token (cf. Beltagy et al. 2020)\n",
    "global_attention_mask = torch.zeros_like(inputs)\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "# Generate Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "31d3d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kayhan/opt/anaconda3/envs/pendulum/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " marufuku is one of my favorite ramen spots in the bay area. \n",
      " it is located in the japantown mall and has a long wait, but it is conveniently located in a shopping area so there is a ton to do while you wait. \n",
      " we ordered the hakata tonkotsu and the chicken paitan. \n",
      " both ramen were (5/5 ) exceptionally crafted with chewy/QQ perfectly cooked noodles. \n",
      " the broth was rich, complex and thick.    \n",
      " # 1_#1 _ # 1_#1 _ =    # 1 1.25 in.125 in.25 in    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    #\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask, num_beams=5,num_return_sequences=3)\n",
    "print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eee28015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " marufuku is one of my favorite ramen spots in the bay area. \n",
      " it is located in the japantown mall and has a long wait, but it is conveniently located in a shopping area so there is a ton to do while you wait. \n",
      " we ordered the hakata tonkotsu and the chicken paitan. \n",
      " both ramen were (5/5 ) exceptionally crafted with chewy/QQ perfectly cooked noodles. \n",
      " the broth was rich, complex and thick.    \n",
      " # 1_#1 _ # 1_#1 _ =    # 1 1.25 in.125 in.25 in    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1 1.25 in.25 in \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(summary_ids[1], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cf2cfecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " marufuku is one of my favorite ramen spots in the bay area. \n",
      " it is located in the japantown mall and has a long wait, but it is conveniently located in a shopping area so there is a ton to do while you wait. \n",
      " we ordered the hakata tonkotsu and the chicken paitan. \n",
      " both ramen were (5/5 ) exceptionally crafted with chewy/QQ perfectly cooked noodles. \n",
      " the broth was rich, complex and thick.    \n",
      " # 1_#1 _ # 1_#1 _ =    # 1 1.25 in.125 in.25 in    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1_#1 _ =    # 1 1.25 in.25 in.25\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(summary_ids[2], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0af61468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask,\n",
    "#                              num_beams=5, min_length=50,\n",
    "#                              max_length=100,\n",
    "#                             max_time=120)\n",
    "# print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1e75a8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(tokenizer.decode(prediction, skip_special_tokens=True))\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "prediction = model.generate(input_ids)[0]\n",
    "# print(tokenizer.decode(prediction, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "765a731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)\"pytorch_model.bin\";:   0%|           | 0.00/353M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:   3%|  | 10.5M/353M [00:01<00:37, 9.04MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:   6%|  | 21.0M/353M [00:02<00:36, 9.19MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:   9%|▏ | 31.5M/353M [00:03<00:37, 8.68MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  12%|▏ | 41.9M/353M [00:05<00:38, 8.06MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  15%|▎ | 52.4M/353M [00:06<00:37, 7.99MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  18%|▎ | 62.9M/353M [00:07<00:37, 7.65MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  21%|▍ | 73.4M/353M [00:09<00:36, 7.71MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  24%|▍ | 83.9M/353M [00:10<00:35, 7.64MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  27%|▌ | 94.4M/353M [00:11<00:34, 7.58MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  30%|▉  | 105M/353M [00:13<00:31, 7.89MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  33%|▉  | 115M/353M [00:14<00:28, 8.24MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  36%|█  | 126M/353M [00:15<00:26, 8.45MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  39%|█▏ | 136M/353M [00:16<00:25, 8.60MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  42%|█▏ | 147M/353M [00:17<00:24, 8.46MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  45%|█▎ | 157M/353M [00:19<00:24, 7.97MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  48%|█▍ | 168M/353M [00:21<00:27, 6.80MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  51%|█▌ | 178M/353M [00:23<00:26, 6.62MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  53%|█▌ | 189M/353M [00:24<00:24, 6.66MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  56%|█▋ | 199M/353M [00:26<00:23, 6.52MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  59%|█▊ | 210M/353M [00:28<00:22, 6.42MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  62%|█▊ | 220M/353M [00:29<00:19, 6.80MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  65%|█▉ | 231M/353M [00:30<00:17, 6.89MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  68%|██ | 241M/353M [00:33<00:18, 6.12MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  71%|██▏| 252M/353M [00:34<00:16, 6.30MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  74%|██▏| 262M/353M [00:36<00:14, 6.26MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  77%|██▎| 273M/353M [00:38<00:13, 6.10MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  80%|██▍| 283M/353M [00:39<00:10, 6.43MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  83%|██▍| 294M/353M [00:40<00:08, 6.78MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  86%|██▌| 304M/353M [00:42<00:07, 6.65MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  89%|██▋| 315M/353M [00:44<00:06, 6.20MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  92%|██▊| 325M/353M [00:46<00:04, 6.24MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  95%|██▊| 336M/353M [00:48<00:02, 6.06MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";:  98%|██▉| 346M/353M [00:49<00:01, 6.10MB/s]\u001b[A\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 353M/353M [00:50<00:00, 6.93MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)neration_config.json: 100%|█████| 124/124 [00:00<00:00, 9.07kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)olve/main/vocab.json: 100%|█| 1.04M/1.04M [00:00<00:00, 1.10MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)olve/main/merges.txt:   0%|           | 0.00/456k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)olve/main/merges.txt: 100%|████| 456k/456k [00:00<00:00, 875kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 1.36M/1.36M [00:00<00:00, 1.37MB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,LEDForConditionalGeneration,AutoModelForSeq2SeqLM,AutoModelForCausalLM\n",
    "checkpoint = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa677f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "class MongoDBCollection:\n",
    "    def __init__(self, ip_address, database_name, collection_name):\n",
    "        '''\n",
    "        Using ip_address, database_name, collection_name,\n",
    "        initiate the instance's attributes including ip_address,\n",
    "        database_name, collection_name, client, db and collection.\n",
    "\n",
    "        For pymongo, see more details in the following.\n",
    "        https://pymongo.readthedocs.io\n",
    "        '''\n",
    "        self.ip_address = ip_address\n",
    "        self.database_name = database_name\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.client = pymongo.MongoClient(f\"mongodb://{ip_address}:27017/\")\n",
    "        self.db = self.client[database_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "\n",
    "    def return_db(self):\n",
    "        '''\n",
    "        Return db which is the database in the client\n",
    "        '''\n",
    "        return self.db\n",
    "\n",
    "    def return_collection(self):\n",
    "        '''\n",
    "        Return db which belongs to the db.\n",
    "        '''\n",
    "        return self.collection\n",
    "\n",
    "    def return_num_docs(self, query):\n",
    "        '''\n",
    "        Return the number of documents satisfying the given query.\n",
    "        '''\n",
    "        return self.collection.count_documents(query)\n",
    "\n",
    "    def drop_collection(self):\n",
    "        '''\n",
    "        Drop the collection\n",
    "        '''\n",
    "        self.collection.drop()\n",
    "\n",
    "    def find(self, query, projection):\n",
    "        '''\n",
    "        Return an iteratatable using query and projection.\n",
    "        '''\n",
    "        output = self.collection.find(query, projection)\n",
    "        return return_generator(output)\n",
    "\n",
    "    def find_sorted_doc(self, n):\n",
    "        '''\n",
    "        Return an iteratable including \n",
    "        the first \"n\" number of documents including\n",
    "        \"received_datetime\" and \"close_datetime\" \n",
    "        ordered by  \"received_datetime\"in descending order.\n",
    "        '''\n",
    "        return self.collection.find({}, {\"_id\": 0, \"received_datetime\": 1,\n",
    "                                    \"close_datetime\": 1})\\\n",
    "            .sort(\"received_datetime\", -1).limit(n)\n",
    "\n",
    "    def return_call_type_weather(self,\n",
    "                                 call_type_substring, \n",
    "                                 weather_limit):\n",
    "        '''\n",
    "        When \"call_type_original_desc\" includes\n",
    "        \"call_type_substring\" in its value (case insensitive)\n",
    "        and any of its \"value\" in \"weather\" \n",
    "        is greater than or equal to \"weather_limit\", \n",
    "        return an iteratable including its \"_id\", \n",
    "        \"call_type_original_desc\" and \n",
    "        the first satisfying \"weather\" elements\n",
    "        ordered by \"_id\" in ascending order.\n",
    "        '''\n",
    "        return self.collection.find({\"call_type_original_desc\":\n",
    "                                    {\"$regex\": call_type_substring,\n",
    "                                     \"$options\": 'i'},\n",
    "                                    \"weather.value\": {\"$gte\": weather_limit}},\n",
    "                                    {\"call_type_original_desc\": 1,\n",
    "                                    \"weather\": {\"$elemMatch\": {\"value\": {\n",
    "                                        \"$gte\": weather_limit}}}}).sort(\"_id\")\n",
    "    \n",
    "    def frequent_call_type_count(self, n):\n",
    "        '''\n",
    "        Return the \"n\" most frequent \"call_type_original_desc\"\n",
    "        including its \"call_type_original_desc\" and \"count\"\n",
    "        '''\n",
    "        return self.collection.aggregate([{\"$group\": {\n",
    "                                    \"_id\": \"$call_type_original_desc\",\n",
    "                                    \"count\": {\"$sum\": 1}}},\n",
    "                                    {\"$sort\": {\"count\": -1}},\n",
    "                                    {\"$limit\": n},\n",
    "                                    {\"$project\": {\n",
    "                                        \"call_type_original_desc\": \"$_id\",\n",
    "                                        \"count\": \"$count\", \"_id\": 0}}])\n",
    "                        \n",
    "    def daily_frequent_call_type_count(self):\n",
    "        '''\n",
    "        Return the \"count\" and \"max_weather_val\" \n",
    "        (the maximum value in \"weather\" array)\n",
    "        grouped by \"received_date\" \n",
    "        (string value of the date portion in \"received_datetime\")\n",
    "        and \"call_type_original_desc\" \n",
    "        ordered by \"count\" (descending order) \n",
    "        and \"received_date\" (ascending order)\n",
    "        '''\n",
    "        return self.collection.aggregate([\n",
    "\n",
    "{\"$addFields\":{\"received_date\":{\"$dateFromString\":{\"dateString\":\"$received_datetime\"}}}},\n",
    "{\"$set\":{\"received_date\":{\"$dateToString\":{\"date\":\"$received_date\",\"format\":\"%Y-%m-%d\"}}}},\n",
    "{\"$group\":{\"_id\":{\"received_date\":\"$received_date\",\"call_type_original_desc\":\"$call_type_original_desc\"},\"count\":{\"$sum\":1},\"max_weather_val\":{\"$max\":'$weather.value'}}},\n",
    "{\"$project\":{\"received_date\":\"$_id.received_date\",\"call_type_original_desc\":\"$_id.call_type_original_desc\",\"count\":\"$count\",\"max_weather_val\":{\"$max\":\"$max_weather_val\"},\"_id\":0,}},\n",
    "{\"$sort\":{\"count\":-1,\"received_date\":1}}]\n",
    ")\n",
    "\n",
    "    def common_sensitive_call_with_different_final_type(self, n):\n",
    "        '''\n",
    "        For \"sensitive_call\" being true, return the n most \n",
    "        \"call_type_original_desc\" and its \"count\" (number) of \n",
    "        \"call_type_original_desc\" being same as\n",
    "        \"call_type_final_desc\"\n",
    "        ordered by \"count\" in descending order.\n",
    "        '''\n",
    "\n",
    "        return self.collection.aggregate([{\"$match\":{\"sensitive_call\":\"True\"}},\n",
    "{\"$match\":{\"$expr\":{\"$eq\":[\"$call_type_original_desc\",\"$call_type_final_desc\"]}}},\n",
    "{\"$group\":{\"_id\":\"$call_type_original_desc\",\"count\":{\"$sum\":1}}},\n",
    "{\"$sort\":{\"count\":-1}},\n",
    "{\"$project\":{\"call_type_original_desc\":\"$_id\",\"count\":\"$count\",\"_id\":0}},\n",
    "{\"$limit\":n}\n",
    "])\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
