{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kayhan/opt/anaconda3/envs/pendulum/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-20 23:30:45.548946: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "import pandas as pd \n",
    "from datasets import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Types of Experiments\n",
    "\n",
    "1. gpt style prompting \n",
    "2. just concattenating reviews\n",
    "3. thorough reviews \n",
    "4. adding more meta data to the prompts \n",
    "\n",
    "\n",
    "## Types of Models to Run Experiments on\n",
    "1. BART base\n",
    "2. GPT2 \n",
    "3. BART Large\n",
    "4. T5\n",
    "5. Trying Distil models (ex: sshleifer/distilbart-cnn-12-6)\n",
    "\n",
    "Other models:\n",
    "BART, BigBird-Pegasus, Blenderbot, BlenderbotSmall, Encoder decoder, FairSeq Machine-Translation, GPTSAN-japanese, LED, LongT5, M2M100, Marian, mBART, MT5, MVP, NLLB, NLLB-MOE, Pegasus, PEGASUS-X, PLBart, ProphetNet, SwitchTransformers, T5, XLM-ProphetNet\n",
    "\n",
    "\n",
    "## Experiment 1. Adding gpt style prompting for BART with the following structure:\n",
    "\n",
    "\"The following are the highest rated reviews of a restaurant on Yelp. Generate a summary that details the opinions about this restaurant:\n",
    "\n",
    "1. 'Great experience at this new 3.5 month old Korean/Thai fusion gem that offered delicious food, warm, friendly service, and a nice comfortable ambiance. Adam, our server, was very welcoming, fun, engaging, helpful with menu recommendations, and thoughtful to check in on us regularly. The items we ordered were all wonderful and flavorful - the hamachi Kama, Korean spicy noodle with pork jowl, Bibimbap with beef brisket, lavender lemonade, and apple pie cheese cake. Do yourself a favor and ask for their house made chili crisp oil and sauces - so so good. Go and enjoy a great vibe, meal, and environment.'\n",
    "2. 'Everything here is excellent. My favorites are the Beef Noodle Soup, Korean Spicy Noodle and the newly added salmon green curry. Great gluten-free options too!'\n",
    "3. ....\n",
    "4. ...\n",
    "5. ...\n",
    "\n",
    "We do the first 5 reviews instead of 10 because the model only has a max length\n",
    "\n",
    "\n",
    "## Experiment 2. ADdding more meta data to prompts \n",
    "\n",
    "Review Set:\n",
    "\n",
    "Restaurant Name: The Blue Door\n",
    "\n",
    "Location: San Francisco, CA\n",
    "\n",
    "Cuisine: American\n",
    "\n",
    "Review 1: \"The Blue Door is a must-visit if you're in San Francisco. The food is outstanding, with a great selection of American classics and some unique dishes. The service is also top-notch, with friendly and attentive staff.\"\n",
    "\n",
    "Review 2: \"I was blown away by the food at The Blue Door. Every dish we tried was amazing, and the drinks were excellent too. The service was also great - our server was very knowledgeable about the menu and made some great recommendations.\"\n",
    "\n",
    "Review 3: \"I've been to The Blue Door several times now and it never disappoints. The food is consistently excellent and the service is always friendly and attentive. I highly recommend the fried chicken and the mac and cheese.\"\n",
    "\n",
    "Review 4: \"The Blue Door has some of the best food I've ever had. The flavors are so well-balanced and everything is cooked to perfection. The service is also excellent - our server was very friendly and made us feel welcome.\"\n",
    "\n",
    "Review 5: \"This place is a hidden gem. The food is outstanding and the atmosphere is cozy and inviting. I highly recommend the beef brisket and the apple pie.\"\n",
    "\n",
    "Prompt:\n",
    "\"Summarize the top 5 reviews for The Blue Door in San Francisco, CA. The restaurant serves American cuisine and is known for its outstanding food and top-notch service.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/summarized_reviews_san_francisco.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperator = '_______JOINED_THE_MESSAGE_ON THIS_STRING_______'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_prompt_type(review_type):\n",
    "    if review_type == 'top_rated':\n",
    "        top_rated_prompt = \"The following are the highest rated reviews of a restaurant on Yelp. Generate a summary that details the opinions about this restaurant:\\n\\n\"\n",
    "        return top_rated_prompt\n",
    "    \n",
    "    elif review_type == \"low_rated\":\n",
    "        low_rated_prompt = \"The following are the lowest rated reviews of a restaurant on Yelp. Generate a summary that details the opinions about this restaurant:\\n\\n\"\n",
    "        return low_rated_prompt\n",
    "    \n",
    "    elif review_type == \"newest_rated\":\n",
    "        newest_rated_prompt = \"The following are the newest rated reviews of a restaurant on Yelp. Generate a summary that details the opinions about this restaurant\\n\\n\"\n",
    "        return newest_rated_prompt\n",
    "    \n",
    "    elif review_type == \"elited_rated\":\n",
    "    \n",
    "        elite_rated_prompt = \"The following are restaurant Yelp reviews of a restaurant written by Yelp Elite members. Generate a summary that details the opinions about this restaurant:\\n\\n\"\n",
    "        return elite_rated_prompt\n",
    "    \n",
    "    else:\n",
    "        return \"The following are restaurant Yelp reviews of a restaurant. Generate a summary that details the opinions about this restaurant:\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        prompt = determine_prompt_type(row['review_type'])\n",
    "        for i_, review in enumerate(row['review']):\n",
    "            if i_ > 5:\n",
    "                break\n",
    "            prompt += f'{i+1}. {review} \\n'\n",
    "        df.loc[i,'prompt'] = prompt\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_prompts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df[['name','review_type','prompt','summary']]],test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'review_type', 'prompt', 'summary', '__index_level_0__'],\n",
       "        num_rows: 2822\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'review_type', 'prompt', 'summary', '__index_level_0__'],\n",
       "        num_rows: 706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train,split='train')\n",
    "test_dataset = Dataset.from_pandas(df_test,split='test')\n",
    "ds_dict = {'train':train_dataset,'test':test_dataset}\n",
    "dataset = DatasetDict(ds_dict)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/bart-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"prompt\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=150, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.18ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'review_type', 'prompt', 'summary', '__index_level_0__'],\n",
       "        num_rows: 2822\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'review_type', 'prompt', 'summary', '__index_level_0__'],\n",
       "        num_rows: 706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART_v2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pendulum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
